#!/usr/bin/env python3
"""
Claude CLI Main Analyzer
ä¸»åˆ†æå™¨ï¼Œåè°ƒæ‰«æå’Œå­—æ®µæå–å·¥ä½œ
"""

import os
import json
import ujson
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import pandas as pd

from core.session_scanner import SessionScanner, ScanResult
from core.field_extractor import FieldExtractor, ExtractionResult, FieldInfo


class MainAnalyzer:
    """ä¸»åˆ†æå™¨"""
    
    def __init__(self, output_dir: str = None):
        self.scanner = SessionScanner()
        self.field_extractor = FieldExtractor()
        self.output_dir = output_dir or "./outputs"
        self.ensure_output_directories()
        
    def ensure_output_directories(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        for subdir in ["sessions", "analysis", "fields", "reports"]:
            os.makedirs(os.path.join(self.output_dir, subdir), exist_ok=True)
    
    def run_complete_analysis(self, max_records_per_file: int = None) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ å¼€å§‹Claude CLIå®Œæ•´åˆ†æ...")
        
        # ç¬¬1æ­¥ï¼šæ‰«ææ‰€æœ‰ä¼šè¯æ–‡ä»¶
        print("\n=== ç¬¬1æ­¥ï¼šæ‰«æä¼šè¯æ–‡ä»¶ ===")
        scan_result = self.scanner.scan_all_sessions()
        self.save_scan_result(scan_result)
        
        # ç¬¬2æ­¥ï¼šæå–å­—æ®µä¿¡æ¯
        print("\n=== ç¬¬2æ­¥ï¼šæå–å­—æ®µä¿¡æ¯ ===")
        extraction_result = self.extract_all_fields(scan_result, max_records_per_file)
        self.save_extraction_result(extraction_result)
        
        # ç¬¬3æ­¥ï¼šç”Ÿæˆåˆ†ææŠ¥å‘Š
        print("\n=== ç¬¬3æ­¥ï¼šç”Ÿæˆåˆ†ææŠ¥å‘Š ===")
        analysis_report = self.generate_analysis_report(scan_result, extraction_result)
        self.save_analysis_report(analysis_report)
        
        print("\nâœ… å®Œæ•´åˆ†æå®Œæˆ!")
        return analysis_report
    
    def extract_all_fields(self, scan_result: ScanResult, max_records_per_file: int = None) -> ExtractionResult:
        """ä»æ‰€æœ‰ä¼šè¯æ–‡ä»¶ä¸­æå–å­—æ®µ"""
        print(f"ğŸ“Š å¼€å§‹å­—æ®µæå–ï¼Œå…± {scan_result.total_files} ä¸ªæ–‡ä»¶...")
        
        total_processed = 0
        
        for i, session_file in enumerate(scan_result.session_files, 1):
            print(f"[{i}/{scan_result.total_files}] å¤„ç†: {session_file.file_path}")
            
            if session_file.file_type == "jsonl":
                processed = self.field_extractor.process_jsonl_file(
                    session_file.file_path, 
                    max_records_per_file
                )
            elif session_file.file_type == "json":
                processed = self.field_extractor.process_json_file(session_file.file_path)
            else:
                print(f"  è·³è¿‡æœªçŸ¥æ–‡ä»¶ç±»å‹: {session_file.file_type}")
                continue
            
            total_processed += processed
            print(f"  âœ“ å·²å¤„ç† {processed} æ¡è®°å½•")
        
        print(f"\nğŸ“ˆ å­—æ®µæå–å®Œæˆï¼Œæ€»å…±å¤„ç† {total_processed} æ¡è®°å½•")
        self.field_extractor.print_field_summary()
        
        return self.field_extractor.generate_extraction_result()
    
    def generate_analysis_report(self, scan_result: ScanResult, extraction_result: ExtractionResult) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        # æ„å»ºå­—æ®µè§„èŒƒè¡¨
        field_specifications = []
        for field_path, field_info in extraction_result.field_registry.items():
            # è®¡ç®—å‡ºç°é¢‘ç‡
            frequency = (field_info.occurrence_count / extraction_result.total_records_processed * 100) if extraction_result.total_records_processed > 0 else 0
            
            # å‡†å¤‡ç¤ºä¾‹å€¼
            examples = field_info.value_examples[:3] if field_info.value_examples else []
            examples_str = ", ".join([f'"{ex}"' if isinstance(ex, str) else str(ex) for ex in examples])
            
            field_specifications.append({
                "å­—æ®µå": field_path,
                "ç±»å‹": field_info.data_type,
                "æè¿°": self.generate_field_description(field_path, field_info),
                "ç¤ºä¾‹": examples_str,
                "å‡ºç°é¢‘ç‡": f"{frequency:.1f}%",
                "å‡ºç°æ¬¡æ•°": field_info.occurrence_count,
                "æ˜¯å¦æšä¸¾": "æ˜¯" if field_info.is_enum else "å¦",
                "æšä¸¾å€¼": field_info.enum_values if field_info.is_enum else None,
                "ç©ºå€¼æ¬¡æ•°": field_info.null_count,
                "å€¼æ¨¡å¼": ", ".join(field_info.value_patterns) if field_info.value_patterns else ""
            })
        
        # æŒ‰å‡ºç°é¢‘ç‡æ’åº
        field_specifications.sort(key=lambda x: x["å‡ºç°æ¬¡æ•°"], reverse=True)
        
        # æ„å»ºæŠ¥å‘Š
        report = {
            "ç”Ÿæˆæ—¶é—´": datetime.now().isoformat(),
            "æ‰«æç»Ÿè®¡": {
                "æ€»æ–‡ä»¶æ•°": scan_result.total_files,
                "æ€»è®°å½•æ•°": scan_result.total_records,
                "æ€»å¤§å°MB": round(scan_result.total_size_bytes / 1024 / 1024, 2),
                "é¡¹ç›®æ•°": len(scan_result.projects),
                "æ—¶é—´èŒƒå›´": {
                    "å¼€å§‹": scan_result.date_range[0].isoformat() if scan_result.date_range else None,
                    "ç»“æŸ": scan_result.date_range[1].isoformat() if scan_result.date_range else None
                }
            },
            "å­—æ®µåˆ†æ": {
                "å‘ç°å­—æ®µæ€»æ•°": extraction_result.total_fields_discovered,
                "å¤„ç†è®°å½•æ€»æ•°": extraction_result.total_records_processed,
                "æšä¸¾å­—æ®µæ•°": len(extraction_result.enum_fields),
                "æ•°æ®ç±»å‹åˆ†å¸ƒ": extraction_result.data_type_distribution
            },
            "å­—æ®µè§„èŒƒ": field_specifications,
            "æšä¸¾å­—æ®µå®Œæ•´åˆ—è¡¨": extraction_result.enum_fields,
            "é¡¹ç›®åˆ†å¸ƒ": scan_result.projects
        }
        
        return report
    
    def generate_field_description(self, field_path: str, field_info: FieldInfo) -> str:
        """ç”Ÿæˆå­—æ®µæè¿°"""
        descriptions = {
            "sessionId": "ä¼šè¯å”¯ä¸€æ ‡è¯†ç¬¦",
            "uuid": "è®°å½•å”¯ä¸€æ ‡è¯†ç¬¦", 
            "parentUuid": "çˆ¶çº§è®°å½•UUID",
            "timestamp": "è®°å½•æ—¶é—´æˆ³",
            "type": "è®°å½•ç±»å‹",
            "message.role": "æ¶ˆæ¯è§’è‰²",
            "message.content": "æ¶ˆæ¯å†…å®¹",
            "message.id": "æ¶ˆæ¯ID",
            "message.model": "ä½¿ç”¨çš„æ¨¡å‹",
            "message.type": "æ¶ˆæ¯ç±»å‹",
            "cwd": "å½“å‰å·¥ä½œç›®å½•",
            "version": "Claude Codeç‰ˆæœ¬",
            "gitBranch": "Gitåˆ†æ”¯",
            "userType": "ç”¨æˆ·ç±»å‹",
            "isSidechain": "æ˜¯å¦ä¸ºä¾§é“¾è®°å½•",
            "isMeta": "æ˜¯å¦ä¸ºå…ƒæ•°æ®",
            "requestId": "APIè¯·æ±‚ID",
            "isVisibleInTranscriptOnly": "æ˜¯å¦ä»…åœ¨è½¬å½•ä¸­å¯è§"
        }
        
        # å°è¯•åŒ¹é…å·²çŸ¥æè¿°
        for pattern, desc in descriptions.items():
            if pattern in field_path:
                return desc
        
        # åŸºäºå­—æ®µè·¯å¾„æ¨æ–­æè¿°
        if "tool_use" in field_path:
            return "å·¥å…·ä½¿ç”¨ç›¸å…³"
        elif "content" in field_path:
            return "å†…å®¹ç›¸å…³"
        elif "usage" in field_path:
            return "ä½¿ç”¨ç»Ÿè®¡ç›¸å…³"
        elif field_info.is_enum:
            return f"æšä¸¾å€¼ï¼Œå¯é€‰: {', '.join(map(str, field_info.enum_values))}"
        else:
            return f"{field_info.data_type}ç±»å‹å­—æ®µ"
    
    def save_scan_result(self, scan_result: ScanResult):
        """ä¿å­˜æ‰«æç»“æœ"""
        output_file = os.path.join(self.output_dir, "sessions", "scan_result.json")
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_data = {
            "total_files": scan_result.total_files,
            "total_records": scan_result.total_records,
            "total_size_bytes": scan_result.total_size_bytes,
            "projects": scan_result.projects,
            "date_range": [
                scan_result.date_range[0].isoformat(),
                scan_result.date_range[1].isoformat()
            ] if scan_result.date_range else None,
            "session_files": [
                {
                    "file_path": sf.file_path,
                    "file_size": sf.file_size,
                    "session_id": sf.session_id,
                    "project_path": sf.project_path,
                    "last_modified": sf.last_modified.isoformat(),
                    "record_count": sf.record_count,
                    "file_type": sf.file_type
                }
                for sf in scan_result.session_files
            ]
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            ujson.dump(serializable_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ æ‰«æç»“æœå·²ä¿å­˜: {output_file}")
    
    def save_extraction_result(self, extraction_result: ExtractionResult):
        """ä¿å­˜å­—æ®µæå–ç»“æœ"""
        output_file = os.path.join(self.output_dir, "fields", "extraction_result.json")
        
        # è½¬æ¢å­—æ®µä¿¡æ¯ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        serializable_fields = {}
        for field_path, field_info in extraction_result.field_registry.items():
            serializable_fields[field_path] = {
                "path": field_info.path,
                "data_type": field_info.data_type,
                "value_examples": field_info.value_examples,
                "occurrence_count": field_info.occurrence_count,
                "null_count": field_info.null_count,
                "unique_values": list(field_info.unique_values),
                "is_enum": field_info.is_enum,
                "enum_values": field_info.enum_values,
                "value_patterns": field_info.value_patterns
            }
        
        serializable_data = {
            "total_records_processed": extraction_result.total_records_processed,
            "total_fields_discovered": extraction_result.total_fields_discovered,
            "data_type_distribution": extraction_result.data_type_distribution,
            "enum_fields": extraction_result.enum_fields,
            "field_registry": serializable_fields
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            ujson.dump(serializable_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ å­—æ®µæå–ç»“æœå·²ä¿å­˜: {output_file}")
    
    def save_analysis_report(self, report: Dict[str, Any]):
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        # ä¿å­˜å®Œæ•´JSONæŠ¥å‘Š
        json_file = os.path.join(self.output_dir, "reports", "analysis_report.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            ujson.dump(report, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜å­—æ®µè§„èŒƒè¡¨ä¸ºCSV
        csv_file = os.path.join(self.output_dir, "reports", "field_specifications.csv")
        df = pd.DataFrame(report["å­—æ®µè§„èŒƒ"])
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        # ä¿å­˜æšä¸¾å­—æ®µåˆ—è¡¨
        enum_file = os.path.join(self.output_dir, "reports", "enum_fields.json")
        with open(enum_file, 'w', encoding='utf-8') as f:
            ujson.dump(report["æšä¸¾å­—æ®µå®Œæ•´åˆ—è¡¨"], f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ åˆ†ææŠ¥å‘Šå·²ä¿å­˜:")
        print(f"   JSONæŠ¥å‘Š: {json_file}")
        print(f"   å­—æ®µè§„èŒƒè¡¨: {csv_file}")
        print(f"   æšä¸¾å­—æ®µ: {enum_file}")


if __name__ == "__main__":
    analyzer = MainAnalyzer()
    report = analyzer.run_complete_analysis(max_records_per_file=1000)  # é™åˆ¶æ¯ä¸ªæ–‡ä»¶å¤„ç†1000æ¡è®°å½•ä»¥æé«˜é€Ÿåº¦