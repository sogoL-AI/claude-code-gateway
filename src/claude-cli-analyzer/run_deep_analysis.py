#!/usr/bin/env python3
"""
è¿è¡Œæ·±åº¦Claude CLIå­—æ®µåˆ†æï¼Œä¸“é—¨æ•è·æ·±å±‚åµŒå¥—å­—æ®µ
"""

import sys
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
src_dir = Path(__file__).parent / "src"
sys.path.insert(0, str(src_dir))

from analyzers.main_analyzer import MainAnalyzer
from core.field_extractor import FieldExtractor


def run_deep_analysis():
    print("ğŸ” å¯åŠ¨æ·±åº¦å­—æ®µåˆ†æï¼Œä¸“é—¨æŸ¥æ‰¾subagentç­‰æ·±å±‚å­—æ®µ...")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(__file__).parent / "outputs"
    
    # åˆ›å»ºå¢å¼ºçš„å­—æ®µåˆ†æå™¨
    analyzer = MainAnalyzer(output_dir=str(output_dir))
    
    # é‡æ–°è¿è¡Œå­—æ®µæå–ï¼Œé‡ç‚¹å…³æ³¨æ·±å±‚åµŒå¥—
    print("\nğŸš€ é‡æ–°æ‰«æå’Œåˆ†æ...")
    scan_result = analyzer.scanner.scan_all_sessions()
    
    # åˆ›å»ºæ–°çš„å­—æ®µæå–å™¨ï¼Œä¸“é—¨å¤„ç†æ·±å±‚åµŒå¥—
    deep_extractor = FieldExtractor(max_unique_values=100, max_examples=20)
    
    print(f"\nğŸ” æ·±åº¦æå–å­—æ®µï¼Œé‡ç‚¹å…³æ³¨å·¥å…·inputå’Œsubagent...")
    
    processed_records = 0
    subagent_records = 0
    tool_use_records = 0
    
    for i, session_file in enumerate(scan_result.session_files, 1):
        if session_file.file_type != "jsonl":
            continue
            
        print(f"[{i}/{scan_result.total_files}] æ·±åº¦åˆ†æ: {session_file.session_id}")
        
        try:
            with open(session_file.file_path, 'r', encoding='utf-8') as f:
                for line_no, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    
                    try:
                        import ujson
                        record = ujson.loads(line)
                        
                        # æ·±åº¦é€’å½’æå–å­—æ®µï¼Œå¢åŠ é€’å½’æ·±åº¦
                        deep_extractor.extract_fields_from_value(record, max_depth=25)
                        processed_records += 1
                        
                        # ç‰¹åˆ«æ£€æŸ¥subagentç›¸å…³è®°å½•
                        record_str = str(record).lower()
                        if 'subagent' in record_str:
                            subagent_records += 1
                            
                        if 'tool_use' in record_str:
                            tool_use_records += 1
                        
                        if processed_records % 5000 == 0:
                            print(f"    å·²å¤„ç† {processed_records} æ¡è®°å½•...")
                            
                    except Exception as e:
                        continue
                        
        except Exception as e:
            print(f"  æ–‡ä»¶å¤„ç†é”™è¯¯: {e}")
            continue
    
    print(f"\nğŸ“Š æ·±åº¦åˆ†æç»Ÿè®¡:")
    print(f"   æ€»å¤„ç†è®°å½•: {processed_records}")
    print(f"   åŒ…å«subagentçš„è®°å½•: {subagent_records}")
    print(f"   åŒ…å«tool_useçš„è®°å½•: {tool_use_records}")
    
    # ç”Ÿæˆæ·±åº¦åˆ†æç»“æœ
    deep_result = deep_extractor.generate_extraction_result()
    
    print(f"\nğŸ¯ æ·±åº¦å­—æ®µå‘ç°:")
    print(f"   æ€»å­—æ®µæ•°: {deep_result.total_fields_discovered}")
    print(f"   æšä¸¾å­—æ®µæ•°: {len(deep_result.enum_fields)}")
    
    # ç‰¹åˆ«å…³æ³¨subagentç›¸å…³å­—æ®µ
    subagent_fields = {}
    tool_input_fields = {}
    
    for field_path, field_info in deep_result.field_registry.items():
        if 'subagent' in field_path.lower():
            subagent_fields[field_path] = field_info
        if 'input.' in field_path and 'tool_use' in field_path:
            tool_input_fields[field_path] = field_info
    
    print(f"\nğŸ¤– Subagentç›¸å…³å­—æ®µ ({len(subagent_fields)}ä¸ª):")
    for field_path in sorted(subagent_fields.keys()):
        field_info = subagent_fields[field_path]
        if field_info.is_enum:
            print(f"   â€¢ {field_path} [{field_info.data_type}]: {field_info.enum_values}")
        else:
            print(f"   â€¢ {field_path} [{field_info.data_type}]: {field_info.occurrence_count}æ¬¡")
    
    print(f"\nâš™ï¸ å·¥å…·Inputå‚æ•°å­—æ®µ (å‰20ä¸ª):")
    tool_input_sorted = sorted(tool_input_fields.items(), key=lambda x: x[1].occurrence_count, reverse=True)
    for field_path, field_info in tool_input_sorted[:20]:
        if field_info.is_enum:
            print(f"   â€¢ {field_path} [{field_info.data_type}]: {field_info.enum_values}")
        else:
            print(f"   â€¢ {field_path} [{field_info.data_type}]: {field_info.occurrence_count}æ¬¡")
    
    # ä¿å­˜æ·±åº¦åˆ†æç»“æœ
    deep_output_file = output_dir / "fields" / "deep_extraction_result.json"
    import ujson
    
    serializable_fields = {}
    for field_path, field_info in deep_result.field_registry.items():
        serializable_fields[field_path] = {
            "path": field_info.path,
            "data_type": field_info.data_type,
            "value_examples": field_info.value_examples,
            "occurrence_count": field_info.occurrence_count,
            "null_count": field_info.null_count,
            "unique_values": list(field_info.unique_values),
            "is_enum": field_info.is_enum,
            "enum_values": field_info.enum_values,
            "value_patterns": field_info.value_patterns
        }
    
    deep_serializable_data = {
        "total_records_processed": processed_records,
        "total_fields_discovered": deep_result.total_fields_discovered,
        "subagent_records_found": subagent_records,
        "tool_use_records_found": tool_use_records,
        "data_type_distribution": deep_result.data_type_distribution,
        "enum_fields": deep_result.enum_fields,
        "field_registry": serializable_fields,
        "subagent_fields": {k: v.enum_values if v.is_enum else v.occurrence_count for k, v in subagent_fields.items()},
        "tool_input_fields": {k: v.enum_values if v.is_enum else v.occurrence_count for k, v in tool_input_fields.items()}
    }
    
    with open(deep_output_file, 'w', encoding='utf-8') as f:
        ujson.dump(deep_serializable_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ æ·±åº¦åˆ†æç»“æœå·²ä¿å­˜: {deep_output_file}")
    
    return deep_serializable_data


if __name__ == "__main__":
    result = run_deep_analysis()