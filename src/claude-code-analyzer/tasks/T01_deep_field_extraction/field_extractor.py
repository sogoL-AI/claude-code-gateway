#!/usr/bin/env python3
"""
T01: æ·±åº¦å­—æ®µæå–åˆ†æä»»åŠ¡
é€’å½’æå–JSONå­—æ®µåˆ°æœ€æ·±å±‚ï¼Œå®ç°204å­—æ®µå»é‡
"""

import sys
import os
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set, Any
from collections import defaultdict, Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.insert(0, str(project_root))

from shared.models import FieldInfo, AnalysisResult
from shared.utils import setup_logging


class FieldExtractor:
    """æ·±åº¦å­—æ®µæå–å™¨"""
    
    def __init__(self, max_examples: int = 10, max_value_length: int = 100):
        self.max_examples = max_examples
        self.max_value_length = max_value_length
        self.fields: Dict[str, FieldInfo] = {}
        self.total_records = 0
        self.total_files = 0
        self.logger = setup_logging("T01_FieldExtractor")
        
    def extract_from_record(self, record: Dict[str, Any]) -> None:
        """ä»å•æ¡è®°å½•ä¸­æå–å­—æ®µ"""
        self.total_records += 1
        self._extract_recursive(record, "")
        
    def _extract_recursive(self, value: Any, path: str, depth: int = 20) -> None:
        """é€’å½’æå–å­—æ®µ"""
        if depth <= 0:
            return
            
        if isinstance(value, dict):
            if path:  # è®°å½•å¯¹è±¡æœ¬èº«
                self._add_field(path, value)
                
            for key, val in value.items():
                new_path = f"{path}.{key}" if path else key
                self._add_field(new_path, val)
                self._extract_recursive(val, new_path, depth - 1)
                
        elif isinstance(value, list):
            if path:  # è®°å½•æ•°ç»„æœ¬èº«
                self._add_field(path, value)
                
            if value:
                # å¤„ç†æ•°ç»„å…ƒç´ 
                for i, item in enumerate(value[:10]):  # åˆ†æå‰10ä¸ªå…ƒç´ 
                    if isinstance(item, (dict, list)):
                        # ä½¿ç”¨ç´¢å¼•è·¯å¾„
                        indexed_path = f"{path}[{i}]"
                        self._extract_recursive(item, indexed_path, depth - 1)
                        
                        # åŒæ—¶ä½¿ç”¨é€šç”¨è·¯å¾„ï¼ˆä»…ç¬¬ä¸€ä¸ªå…ƒç´ ï¼‰
                        if i == 0:
                            generic_path = f"{path}[*]"
                            self._extract_recursive(item, generic_path, depth - 1)
                    else:
                        # åŸºæœ¬ç±»å‹æ•°ç»„å…ƒç´ 
                        element_path = f"{path}[*]"
                        self._add_field(element_path, item)
                        
    def _add_field(self, path: str, value: Any) -> None:
        """æ·»åŠ å­—æ®µä¿¡æ¯"""
        if path not in self.fields:
            self.fields[path] = FieldInfo(
                path=path,
                data_type=self._get_type(value),
                examples=[],
                unique_values=set()
            )
            
        field = self.fields[path]
        field.count += 1
        
        if value is None:
            field.null_count += 1
            return
            
        # æ›´æ–°æ•°æ®ç±»å‹
        current_type = self._get_type(value)
        field.data_type = self._merge_types(field.data_type, current_type)
        
        # æ·»åŠ ç¤ºä¾‹å€¼
        if len(field.examples) < self.max_examples:
            truncated = self._truncate_value(value)
            if truncated not in field.examples:
                field.examples.append(truncated)
                
        # æ”¶é›†å”¯ä¸€å€¼ï¼ˆä»…åŸºæœ¬ç±»å‹ï¼‰
        if isinstance(value, (str, int, float, bool)) and len(field.unique_values) < 50:
            field.unique_values.add(value)
            
        # æ£€æŸ¥æ˜¯å¦ä¸ºæšä¸¾
        if (field.count >= 5 and 
            len(field.unique_values) <= min(50, field.count * 0.8) and
            field.data_type in ["string", "integer", "boolean"]):
            field.is_enum = True
            field.enum_values = sorted(list(field.unique_values))
            
    def _get_type(self, value: Any) -> str:
        """è·å–æ•°æ®ç±»å‹"""
        if value is None:
            return "null"
        elif isinstance(value, bool):
            return "boolean"  
        elif isinstance(value, int):
            return "integer"
        elif isinstance(value, float):
            return "number"
        elif isinstance(value, str):
            return self._get_string_type(value)
        elif isinstance(value, list):
            return "array" if value else "array[empty]"
        elif isinstance(value, dict):
            return "object"
        else:
            return "unknown"
            
    def _get_string_type(self, value: str) -> str:
        """è·å–å­—ç¬¦ä¸²å­ç±»å‹"""
        if re.match(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', value, re.I):
            return "uuid"
        elif re.match(r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(\\.\\d{3})?Z?$', value):
            return "datetime"
        elif value.startswith(('http://', 'https://')):
            return "url"
        elif value.startswith(('req_', 'toolu_', 'msg_')):
            return "id"
        else:
            return "string"
            
    def _merge_types(self, type1: str, type2: str) -> str:
        """åˆå¹¶æ•°æ®ç±»å‹"""
        if type1 == type2:
            return type1
        if type1 == "null":
            return type2
        if type2 == "null":
            return type1
        if {type1, type2} <= {"integer", "number"}:
            return "number"
        if {type1, type2} <= {"string", "uuid", "datetime", "url", "id"}:
            return "string"
        return "mixed"
        
    def _truncate_value(self, value: Any) -> Any:
        """æˆªæ–­è¿‡é•¿å€¼"""
        if isinstance(value, str) and len(value) > self.max_value_length:
            return value[:self.max_value_length] + "..."
        elif isinstance(value, (dict, list)):
            str_value = str(value)
            if len(str_value) > self.max_value_length:
                return str_value[:self.max_value_length] + "..."
            return str_value
        return value
        
    def merge_array_fields(self) -> None:
        """åˆå¹¶æ•°ç»„ç´¢å¼•å­—æ®µï¼Œå¦‚ content[0].title + content[1].title -> content[*].title"""
        self.logger.info("åˆå¹¶æ•°ç»„ç´¢å¼•å­—æ®µ...")
        
        # æŒ‰æ¨¡å¼åˆ†ç»„å­—æ®µ
        pattern_groups = defaultdict(list)
        for field_path in list(self.fields.keys()):
            # å°† [æ•°å­—] å’Œ [] éƒ½æ›¿æ¢ä¸º [*]
            normalized = re.sub(r'\\[\\d*\\]', '[*]', field_path)
            if '[*]' in normalized and normalized != field_path:
                pattern_groups[normalized].append(field_path)
                
        merged_count = 0
        for normalized_path, similar_fields in pattern_groups.items():
            if len(similar_fields) <= 1:
                continue
                
            # åˆ›å»ºåˆå¹¶å­—æ®µ
            merged_field = FieldInfo(
                path=normalized_path,
                data_type="mixed",
                examples=[],
                unique_values=set()
            )
            
            # åˆå¹¶ä¿¡æ¯
            all_examples = []
            data_types = set()
            
            for field_path in similar_fields:
                field = self.fields[field_path]
                merged_field.count += field.count
                merged_field.null_count += field.null_count
                data_types.add(field.data_type)
                all_examples.extend(field.examples)
                merged_field.unique_values.update(field.unique_values)
                del self.fields[field_path]
                
            # è®¾ç½®åˆå¹¶åçš„ç±»å‹
            merged_field.data_type = list(data_types)[0] if len(data_types) == 1 else "mixed"
            
            # å»é‡ç¤ºä¾‹
            seen = set()
            for example in all_examples:
                if len(merged_field.examples) >= self.max_examples:
                    break
                example_str = str(example)
                if example_str not in seen:
                    seen.add(example_str)
                    merged_field.examples.append(example)
                    
            self.fields[normalized_path] = merged_field
            merged_count += len(similar_fields)
            
        self.logger.info(f"åˆå¹¶äº† {merged_count} ä¸ªé‡å¤å­—æ®µ")
        
    def process_scan_result(self, scan_result_file: str) -> int:
        """åŸºäºT06æ‰«æç»“æœå¤„ç†æ–‡ä»¶"""
        self.logger.info(f"åŠ è½½æ‰«æç»“æœ: {scan_result_file}")
        
        with open(scan_result_file, 'r', encoding='utf-8') as f:
            scan_data = json.load(f)
        
        processed = 0
        file_details = scan_data.get("file_details", [])
        
        self.logger.info(f"å¼€å§‹å¤„ç† {len(file_details)} ä¸ªæ–‡ä»¶...")
        
        for file_info in file_details:
            file_path = file_info["path"]
            file_type = file_info["file_type"]
            
            count = self._process_file(file_path, file_type)
            processed += count
            self.total_files += 1
            
            if self.total_files % 50 == 0:
                self.logger.info(f"å·²å¤„ç† {self.total_files} ä¸ªæ–‡ä»¶, {processed:,} æ¡è®°å½•")
        
        return processed
        
    def _process_file(self, file_path: str, file_type: str) -> int:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        processed = 0
        
        try:
            if file_type == "jsonl":
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_no, line in enumerate(f, 1):
                        line = line.strip()
                        if not line:
                            continue
                            
                        try:
                            record = json.loads(line)
                            self.extract_from_record(record)
                            processed += 1
                        except json.JSONDecodeError:
                            continue
                            
            else:  # json
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.extract_from_record(data)
                    processed = 1
                    
        except Exception as e:
            self.logger.warning(f"æ–‡ä»¶å¤„ç†é”™è¯¯ {file_path}: {e}")
            
        return processed
        
    def get_result(self) -> AnalysisResult:
        """è·å–åˆ†æç»“æœ"""
        # åˆå¹¶æ•°ç»„å­—æ®µ
        self.merge_array_fields()
        
        # ç»Ÿè®¡æ•°æ®ç±»å‹åˆ†å¸ƒ
        type_dist = Counter(field.data_type for field in self.fields.values())
        
        return AnalysisResult(
            fields=self.fields,
            total_records=self.total_records,
            total_fields=len(self.fields),
            total_files=self.total_files,
            data_types=dict(type_dist)
        )


def generate_field_outputs(result: AnalysisResult, output_dir: Path):
    """ç”Ÿæˆå­—æ®µåˆ†æè¾“å‡º"""
    
    # 1. ç”Ÿæˆå»é‡å­—æ®µæ¸…å•
    merged_count = sum(1 for field in result.fields.values() if '[*]' in field.path)
    
    deduplicated_output = {
        "ç”Ÿæˆæ—¶é—´": datetime.now().isoformat(),
        "ä»»åŠ¡ä¿¡æ¯": {
            "task_id": "T01",
            "task_name": "æ·±åº¦å­—æ®µæå–åˆ†æ"
        },
        "ç»Ÿè®¡ä¿¡æ¯": {
            "å¤„ç†æ–‡ä»¶æ•°": result.total_files,
            "å¤„ç†è®°å½•æ•°": result.total_records,
            "å»é‡å­—æ®µæ€»æ•°": result.total_fields,
            "åˆå¹¶å­—æ®µæ•°": merged_count
        },
        "å­—æ®µæ¸…å•": {}
    }
    
    # æŒ‰å­—æ®µè·¯å¾„æ’åºï¼Œç”Ÿæˆå­—æ®µæ¸…å• (å­—æ®µè·¯å¾„ä½œä¸ºé”®ï¼Œç¤ºä¾‹å€¼ä½œä¸ºå€¼)
    for field_path, field_info in sorted(result.fields.items()):
        # å–ç¬¬ä¸€ä¸ªç¤ºä¾‹å€¼ä½œä¸ºä»£è¡¨
        if field_info.examples:
            example_value = field_info.examples[0]
        else:
            # å¦‚æœæ²¡æœ‰ç¤ºä¾‹ï¼Œæ ¹æ®ç±»å‹æä¾›é»˜è®¤å€¼
            type_defaults = {
                "string": "",
                "integer": 0,
                "boolean": False,
                "array": [],
                "object": {},
                "null": None
            }
            example_value = type_defaults.get(field_info.data_type, "")
            
        deduplicated_output["å­—æ®µæ¸…å•"][field_path] = example_value
        
    # ä¿å­˜å»é‡å­—æ®µæ¸…å•
    dedupe_file = output_dir / "deduplicated_fields.json"
    with open(dedupe_file, 'w', encoding='utf-8') as f:
        json.dump(deduplicated_output, f, indent=2, ensure_ascii=False)
    
    # 2. ç”Ÿæˆè¯¦ç»†å­—æ®µåˆ†æ
    detailed_output = {
        "ç”Ÿæˆæ—¶é—´": datetime.now().isoformat(),
        "ä»»åŠ¡ä¿¡æ¯": {
            "task_id": "T01", 
            "task_name": "æ·±åº¦å­—æ®µæå–åˆ†æ"
        },
        "ç»Ÿè®¡ä¿¡æ¯": deduplicated_output["ç»Ÿè®¡ä¿¡æ¯"],
        "æ•°æ®ç±»å‹åˆ†å¸ƒ": result.data_types,
        "å­—æ®µè¯¦æƒ…": []
    }
    
    for field_path, field_info in sorted(result.fields.items()):
        field_detail = {
            "å­—æ®µè·¯å¾„": field_path,
            "æ•°æ®ç±»å‹": field_info.data_type,
            "å‡ºç°æ¬¡æ•°": field_info.count,
            "ç©ºå€¼æ¬¡æ•°": field_info.null_count,
            "ç©ºå€¼ç‡": f"{field_info.null_count/field_info.count*100:.1f}%" if field_info.count > 0 else "0%",
            "ç¤ºä¾‹å€¼": field_info.examples,
            "æ˜¯å¦æšä¸¾": field_info.is_enum,
            "æšä¸¾å€¼": field_info.enum_values if field_info.is_enum else None
        }
        detailed_output["å­—æ®µè¯¦æƒ…"].append(field_detail)
    
    # ä¿å­˜è¯¦ç»†åˆ†æ
    detailed_file = output_dir / "field_analysis_detailed.json"
    with open(detailed_file, 'w', encoding='utf-8') as f:
        json.dump(detailed_output, f, indent=2, ensure_ascii=False)
    
    return [dedupe_file, detailed_file]


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("Usage: python field_extractor.py <output_dir>")
        sys.exit(1)
    
    output_dir = Path(sys.argv[1])
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ” T01: æ·±åº¦å­—æ®µæå–åˆ†æä»»åŠ¡")
    print("=" * 50)
    
    # æŸ¥æ‰¾T06çš„æ‰«æç»“æœ
    scan_result_file = output_dir.parent / "T06_data_scan" / "scan_results.json"
    if not scan_result_file.exists():
        print(f"âŒ ä¾èµ–æ–‡ä»¶ä¸å­˜åœ¨: {scan_result_file}")
        print("   è¯·å…ˆæ‰§è¡Œ T06 æ•°æ®æºæ‰«æä»»åŠ¡")
        sys.exit(1)
    
    # æ‰§è¡Œå­—æ®µæå–
    extractor = FieldExtractor()
    processed_records = extractor.process_scan_result(str(scan_result_file))
    
    # è·å–åˆ†æç»“æœ
    result = extractor.get_result()
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶
    output_files = generate_field_outputs(result, output_dir)
    
    print(f"\\nâœ… T01 ä»»åŠ¡å®Œæˆ")
    print(f"ğŸ“Š æå–ç»“æœ:")
    print(f"   å¤„ç†æ–‡ä»¶: {result.total_files}")
    print(f"   å¤„ç†è®°å½•: {result.total_records:,}")
    print(f"   æå–å­—æ®µ: {result.total_fields}")
    print(f"   æ•°ç»„åˆå¹¶: {sum(1 for f in result.fields.values() if '[*]' in f.path)}")
    
    print(f"\\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    for output_file in output_files:
        print(f"   {output_file}")


if __name__ == "__main__":
    main()